 import pandas as pd  # Importing the pandas library and assigning the alias "pd"

# Load the dataset
df_tennis = pd.read_csv("decisiontree.csv")  # Reading the CSV file "decisiontree.csv" and storing the data in the DataFrame df_tennis
print("Given play tennis dataset:")
print(df_tennis)  # Printing the contents of the df_tennis DataFrame
print()

def entropy(probs):
    import math
    return sum([-prob * math.log(prob, 2) for prob in probs])  # Calculating the entropy using the formula -p * log2(p) for each probability in the list

def entropy_of_list(a_list):
    from collections import Counter  # Importing the Counter class from the collections module
    cnt = Counter(x for x in a_list)  # Counting the occurrences of each element in the list
    num_instances = len(a_list)  # Getting the total number of instances in the list
    
    probs = [x / num_instances for x in cnt.values()]  # Calculating the probabilities by dividing the counts by the total number of instances
    
    return entropy(probs)  # Calculating the entropy using the probability list

def information_gain(df, split_attribute_name, target_attribute_name, trace=0):
    df_split = df.groupby(split_attribute_name)  # Grouping the DataFrame df by the split attribute name
    nobs = len(df.index)  # Getting the total number of instances in the DataFrame
    df_agg_cnt = df_split.agg({target_attribute_name: [entropy_of_list, lambda x: len(x) / nobs]})[target_attribute_name]
    # Aggregating the counts and entropy of the target attribute by applying entropy_of_list function and calculating the proportion of observations
    df_agg_cnt.columns = ['Entropy', 'Propobservations']  # Renaming the columns of the aggregated DataFrame
    
    if trace:
        print(df_agg_cnt)  # Printing the aggregated counts and entropy if the trace flag is set
    
    new_entropy = sum(df_agg_cnt['Entropy'] * df_agg_cnt['Propobservations'])  # Calculating the new entropy after the split
    old_entropy = entropy_of_list(df[target_attribute_name])  # Calculating the entropy before the split
    
    return old_entropy - new_entropy  # Calculating the information gain by subtracting the new entropy from the old entropy

total_entropy = entropy_of_list(df_tennis['PlayTennis'])  # Calculating the entropy of the target attribute in the df_tennis DataFrame
print("Total entropy of play tennis dataset:", total_entropy)  # Printing the total entropy of the dataset
print("Info-gain for Outlook is:", information_gain(df_tennis, 'Outlook', 'PlayTennis'))
# Calculating and printing the information gain for the 'Outlook' attribute in the df_tennis DataFrame
print("Info-gain for Humidity is:", information_gain(df_tennis, 'Humidity', 'PlayTennis'))
# Calculating and printing the information gain for the 'Humidity' attribute in the df_tennis DataFrame
print("Info-gain for Wind is:", information_gain(df_tennis, 'Wind', 'PlayTennis'))
# Calculating and printing the information gain for the 'Wind' attribute in the df_tennis DataFrame
print("Info-gain for Temperature is:", information_gain(df_tennis, 'Temperature', 'PlayTennis'))
# Calculating and printing the information gain for the 'Temperature' attribute in the df_tennis DataFrame
print()

def id3(df, target_attribute_name, attribute_names, default_class=None):
    from collections import Counter  # Importing the Counter class from the collections module
    cnt = Counter(x for x in df[target_attribute_name])  # Counting the occurrences of each value in the target attribute
    
    if len(cnt) == 1:  # If there is only one unique value in the target attribute
        return next(iter(cnt))  # Return that value as the predicted class
    elif df.empty or not attribute_names:  # If the DataFrame is empty or there are no remaining attribute names
        return default_class  # Return the default class value
    else:
        default_class = max(cnt.keys())  # Set the default class as the value with the maximum count in the target attribute
        gainz = [information_gain(df, attr, target_attribute_name) for attr in attribute_names]
        # Calculate the information gain for each attribute in attribute_names and store the results in a list
        
        index_of_max = gainz.index(max(gainz))  # Find the index of the attribute with the maximum information gain
        best_attr = attribute_names[index_of_max]  # Get the attribute name with the maximum information gain
        tree = {best_attr: {}}  # Create a tree node with the best attribute as the root
        
        remaining_attribute_names = [i for i in attribute_names if i != best_attr]
        # Create a list of remaining attribute names without the best attribute
        
        for attr_val, data_subset in df.groupby(best_attr):
            # Split the DataFrame into subsets based on the attribute values of the best attribute
            subtree = id3(data_subset, target_attribute_name, remaining_attribute_names, default_class)
            # Recursively call the id3 function on each subset to create subtrees
            tree[best_attr][attr_val] = subtree  # Add the subtree to the main tree
            
        return tree  # Return the generated decision tree

attribute_names = list(df_tennis.columns)  # Get a list of attribute names from the DataFrame columns
attribute_names.remove('PlayTennis')  # Remove the target attribute name from the list

tree = id3(df_tennis, 'PlayTennis', attribute_names)  # Generate the decision tree using the id3 function

print("The resultant Decision Tree is:")
print(tree)  # Print the resulting decision tree
print()
def predict(query, tree, default='No'):
    for key in query.keys():  # Iterate over each attribute in the query
        if key in tree.keys():  # Check if the attribute exists in the decision tree
            try:
                result = tree[key][query[key]]  # Get the result based on the attribute value in the query
            except:
                return default  # If an exception occurs, return the default value
            result = tree[key][query[key]]  # Get the result based on the attribute value in the query

            if isinstance(result, dict):  # Check if the result is a dictionary (indicating a subtree)
                return predict(query, result)  # Recursively call the predict function on the subtree
            else:
                return result  # Return the final predicted class value
    return default  # If no matching attribute is found in the tree, return the default value

query = {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak'}
answer = predict(query, tree)  # Predict the outcome for the given query using the decision tree
print("Can tennis be played for the given sample:", answer)  # Print the predicted outcome
