import pandas as pd

df_tennis = pd.read_csv("decisiontree.csv")
print("Given play tennis dataset:")
print(df_tennis)
print()

def entropy(probs):
    import math
    return sum([-prob * math.log(prob, 2) for prob in probs])

def entropy_of_list(a_list):
    from collections import Counter
    cnt = Counter(x for x in a_list)
    num_instances = len(a_list)
    probs = [x / num_instances for x in cnt.values()]
    return entropy(probs)

def information_gain(df, split_attribute_name, target_attribute_name, trace=0):
    df_split = df.groupby(split_attribute_name)
    nobs = len(df.index)
    df_agg_cnt = df_split.agg({target_attribute_name: [entropy_of_list, lambda x: len(x) / nobs]})[target_attribute_name]
    df_agg_cnt.columns = ['Entropy', 'Propobservations']
    
    if trace:
        print(df_agg_cnt)
    
    new_entropy = sum(df_agg_cnt['Entropy'] * df_agg_cnt['Propobservations'])
    old_entropy = entropy_of_list(df[target_attribute_name])
    
    return old_entropy - new_entropy

total_entropy = entropy_of_list(df_tennis['PlayTennis'])
print("Total entropy of play tennis dataset:", total_entropy)
print("Info-gain for Outlook is:", information_gain(df_tennis, 'Outlook', 'PlayTennis'))
print("Info-gain for Humidity is:", information_gain(df_tennis, 'Humidity', 'PlayTennis'))
print("Info-gain for Wind is:", information_gain(df_tennis, 'Wind', 'PlayTennis'))
print("Info-gain for Temperature is:", information_gain(df_tennis, 'Temperature', 'PlayTennis'))
print()

def id3(df, target_attribute_name, attribute_names, default_class=None):
    from collections import Counter
    cnt = Counter(x for x in df[target_attribute_name])
    
    if len(cnt) == 1:
        return next(iter(cnt))
    elif df.empty or not attribute_names:
        return default_class
    else:
        default_class = max(cnt.keys())
        gainz = [information_gain(df, attr, target_attribute_name) for attr in attribute_names]
        index_of_max = gainz.index(max(gainz))
        best_attr = attribute_names[index_of_max]
        tree = {best_attr: {}}
        
        remaining_attribute_names = [i for i in attribute_names if i != best_attr]
        
        for attr_val, data_subset in df.groupby(best_attr):
            subtree = id3(data_subset, target_attribute_name, remaining_attribute_names, default_class)
            tree[best_attr][attr_val] = subtree
            
        return tree

attribute_names = list(df_tennis.columns)
attribute_names.remove('PlayTennis')

tree = id3(df_tennis, 'PlayTennis', attribute_names)

print("The resultant Decision Tree is:")
print(tree)
print()

def predict(query, tree, default='No'):
    for key in query.keys():
        if key in tree.keys():
            try:
                result = tree[key][query[key]]
            except:
                return default
            result = tree[key][query[key]]

            if isinstance(result, dict):
                return predict(query, result)
            else:
                return result
    return default

query = {'Outlook': 'Sunny', 'Temperature': 'Hot', 'Humidity': 'High', 'Wind': 'Weak'}
answer = predict(query, tree)
print("Can tennis be played for the given sample:", answer)
